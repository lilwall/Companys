{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paginated Export API to Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import urllib.parse\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"API Data Fetch\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page size is about 50000 records and as it's large amount of data, have to filter it so the cluster not crash and have a better performance of the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_intersections = \"https://api/public/v1/models/12121212/\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"authorization\": \"Basic MTIxMjEyMTIxMjExMjE6MTIxMjEyMTEx\"\n",
    "}\n",
    "\n",
    "def fetch_data_by_scenario(scenario, page_size=50000):\n",
    "    \"\"\"\n",
    "    Fetches data for a given scenario from the API and returns a list of Spark DataFrames in smaller sets.\n",
    "    \"\"\"\n",
    "    # Define filters for the scenario\n",
    "    filters = [{\"field\": \"Scenario\", \"eq\": str(scenario)}]\n",
    "    filters_param = urllib.parse.quote(json.dumps(filters))\n",
    "\n",
    "    # Define start URL\n",
    "    next_page_url = f\"{url_intersections}?filters={filters_param}&pageSize={page_size}\"\n",
    "    all_batches = []  # List to store batches as DataFrames\n",
    "    batch_count = 0  # Counter to track batches\n",
    "\n",
    "    while next_page_url:\n",
    "        response = requests.get(next_page_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the data\n",
    "        if \"data\" in data:\n",
    "            df_batch = pd.DataFrame(data[\"data\"])\n",
    "            batch_count += 1\n",
    "\n",
    "            # Convert Pandas DataFrame to Spark DataFrame\n",
    "            spark_df_batch = spark.createDataFrame(df_batch)\n",
    "\n",
    "            # Append the Spark DataFrame batch to the list\n",
    "            all_batches.append(spark_df_batch)\n",
    "            \n",
    "            print(f\"Fetched and added batch {batch_count} with {len(df_batch)} records to memory.\")\n",
    "\n",
    "        else:\n",
    "            print(\"No 'data' field in response, stopping.\")\n",
    "            break\n",
    "\n",
    "        # Update the next page URL\n",
    "        metadata = data.get(\"metadata\", {})\n",
    "        next_page_url = metadata.get(\"nextPage\")\n",
    "        if not next_page_url:\n",
    "            print(\"No more pages to fetch.\")\n",
    "\n",
    "    print(f\"Finished processing scenario '{scenario}' in {batch_count} batches.\")\n",
    "    return all_batches  # Return the list of DataFrame batches\n",
    "\n",
    "# Fetch data for the \"Live\" scenario and store it in memory\n",
    "data_batches = fetch_data_by_scenario(\"Live\")\n",
    "\n",
    "# Example: Combine all batches into a single Spark DataFrame if needed\n",
    "if data_batches:\n",
    "    combined_df = data_batches[0]\n",
    "    for df in data_batches[1:]:\n",
    "        combined_df = combined_df.union(df)\n",
    "\n",
    "# Print schema and preview of the combined DataFrame\n",
    "if data_batches:\n",
    "    print(\"\\nSchema of the combined DataFrame:\")\n",
    "    combined_df.printSchema()\n",
    "\n",
    "    print(\"\\nPreview of the combined DataFrame:\")\n",
    "    combined_df.show(n=5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.head())  # Display the first 5 rows of the df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case had to set up the column names manually but usually are inherited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Specify column names\n",
    "column_names = ['Values', 'Account', 'Entity', 'Department', 'Project', 'Year', 'Period', 'Scenario', 'Curency']  # Replace with your desired column names\n",
    "\n",
    "# Assign the specified column names to the master DataFrame\n",
    "combined_df = combined_df.toDF(*column_names)\n",
    "\n",
    "# Display the updated DataFrame with new column names\n",
    "print(\"\\nPreview of the master DataFrame after assigning column names:\")\n",
    "display(combined_df.limit(5))\n",
    "\n",
    "# Add a new column with the current timestamp, to have records of the data\n",
    "combined_df = combined_df.withColumn('timeseries', current_timestamp())\n",
    "\n",
    "# Print the schema to verify the new column\n",
    "print(\"\\nSchema after adding the 'timeseries' column:\")\n",
    "combined_df.printSchema()\n",
    "\n",
    "# Display the DataFrame to verify the new data\n",
    "print(\"\\nMaster DataFrame after adding the 'timeseries' column:\")\n",
    "display(combined_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define the target table name\n",
    "target_table = \"andrey.default.live\"  # Replace with your table name\n",
    "\n",
    "# Write the data to the table in overwrite mode with schema merging\n",
    "combined_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(target_table)\n",
    "\n",
    "print(f\"Data successfully appended to Delta table: {target_table}\")\n",
    "\n",
    "# Load the table and display the data\n",
    "appended_data = spark.table(target_table)\n",
    "\n",
    "# Show schema and preview of the updated table\n",
    "print(\"\\nSchema of the updated Delta table:\")\n",
    "appended_data.printSchema()\n",
    "\n",
    "print(\"\\nPreview of the updated Delta table:\")\n",
    "appended_data.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query to Check the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "\n",
    "SELECT count(*)\n",
    "FROM andrey.default.live\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM andrey.default.live\n",
    "WHERE WHERE timeseries LIKE '2024-12-03%'\n",
    "LIMIT 100"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
