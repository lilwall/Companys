{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install msal requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-pptx   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install msal requests pandas openpyxl python-pptx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the Python kernel dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pptx import Presentation  \n",
    "import msal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant_id = dbutils.secrets.get(scope=\"company\", key=\"tenant_id\")\n",
    "client_id = dbutils.secrets.get(scope=\"company\", key=\"client_id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"company\", key=\"client_secret_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentication MSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "authority = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n",
    "scope = [\"https://graph.microsoft.com/.default\"]\n",
    "\n",
    "# Set up MSAL authentication\n",
    "app = msal.ConfidentialClientApplication(\n",
    "    client_id=client_id,\n",
    "    client_credential=client_secret,\n",
    "    authority=f\"https://login.microsoftonline.com/{tenant_id}\",\n",
    ")\n",
    "\n",
    "# Acquire token or refresh if expired\n",
    "def get_access_token():\n",
    "    token_response = app.acquire_token_silent(scopes=scope, account=None)\n",
    "    \n",
    "    # If no token is available, acquire a new one\n",
    "    if not token_response:\n",
    "        token_response = app.acquire_token_for_client(scopes=scope)\n",
    "\n",
    "    access_token = token_response.get(\"access_token\")\n",
    "    if access_token:\n",
    "        print(\"Access token acquired successfully.\")\n",
    "        return access_token\n",
    "    else:\n",
    "        raise Exception(\"Failed to acquire access token.\")\n",
    "\n",
    "# Get the token\n",
    "access_token = get_access_token()\n",
    "\n",
    "# Set headers with the access token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "#Set up the sharepoint location with spaces and everythin and this cell code will encoded to be processed\n",
    "# Define the SharePoint site and file path and encode it\n",
    "sharepoint_site = 'andrey.sharepoint.com'\n",
    "site_name = 'Production'  \n",
    "file_path = '/Shared Documents/Documents/Volumes.xlsx' \n",
    "#just change the file_path, change the URL, you have to put it as it is, with spaces and slashes and dots.\n",
    "\n",
    "encoded_file_path = urllib.parse.quote(file_path)\n",
    "print(f\"Encoded file path: {encoded_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Retrieve the Site ID we are going to map\n",
    "site_url = f\"https://graph.microsoft.com/v1.0/sites/{sharepoint_site}:/sites/{site_name}\"\n",
    "site_response = requests.get(site_url, headers=headers)\n",
    "\n",
    "if site_response.status_code == 200:\n",
    "    site_data = site_response.json()\n",
    "    site_id = site_data['id']  # Extract the site ID\n",
    "    print(f\"Successfully accessed the SharePoint site with Site ID: {site_id}\")\n",
    "else:\n",
    "    raise Exception(f\"Failed to access SharePoint site: {site_response.status_code}, {site_response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Retrieve the Document Library (Drive ID) this driver it's requested by microsoft graog in the endpoint\n",
    "drive_url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives\"\n",
    "drive_response = requests.get(drive_url, headers=headers)\n",
    "\n",
    "if drive_response.status_code == 200:\n",
    "    drives_data = drive_response.json()\n",
    "    drive_id = None\n",
    "    for drive in drives_data['value']:\n",
    "        if \"Shared Documents\" or \"Documents\" in drive['name']:  # Match against \"Shared Documents\"\n",
    "            drive_id = drive['id']\n",
    "            break\n",
    "    \n",
    "    if drive_id is None:\n",
    "        raise Exception(\"Drive ID for 'Shared Documents' not found.\")\n",
    "    else:\n",
    "        print(f\"Drive ID: {drive_id}\")\n",
    "else:\n",
    "    raise Exception(f\"Failed to retrieve drives: {drive_response.status_code}, {drive_response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the file ID using the search API\n",
    "search_url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/root/search(q='{file_path.split('/')[-1]}')\"\n",
    "search_response = requests.get(search_url, headers=headers)\n",
    "\n",
    "if search_response.status_code == 200:\n",
    "    search_data = search_response.json()\n",
    "    if len(search_data['value']) > 0:\n",
    "        file_id = search_data['value'][0]['id']  # Extract the first matching file's ID\n",
    "        print(f\"Successfully retrieved the File ID: {file_id}\")\n",
    "    else:\n",
    "        raise Exception(\"File not found in search results.\")\n",
    "else:\n",
    "    raise Exception(f\"Failed to search for the file: {search_response.status_code}, {search_response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final endpoint\n",
    "file_content_url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/items/{file_id}/content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the specific sheet from Sharepoint to a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a specific sheet from the Excel file\n",
    "def load_excel_from_sharepoint(file_url, headers, sheet_name=None):\n",
    "    response = requests.get(file_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        file_stream = BytesIO(response.content)\n",
    "        # Specify the sheet_name parameter to select a specific sheet\n",
    "        df = pd.read_excel(file_stream, sheet_name=sheet_name)  \n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f\"Failed to access the file: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Specify the name of the sheet you want to load\n",
    "sheet_name = \"Volume\"  # Replace with the actual sheet name\n",
    "\n",
    "# Load the specified sheet from the Excel file into a DataFrame\n",
    "df_target_sheet = load_excel_from_sharepoint(file_content_url, headers, sheet_name=sheet_name)\n",
    "print(f\"DataFrame loaded from SharePoint (Sheet: {sheet_name}):\")\n",
    "df_target_sheet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform your data if it's necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SaveDeltaTable\").getOrCreate()\n",
    "\n",
    "# Add a new column with the current timestamp as 'timeseries'\n",
    "spark_df_final = spark_df_final.withColumn('snapshot_date', current_timestamp())\n",
    "\n",
    "# Print the updated schema to verify the new column\n",
    "print(\"\\nSchema of the final Spark DataFrame with the 'timeseries' column:\")\n",
    "spark_df_final.printSchema()\n",
    "\n",
    "# Display the updated Spark DataFrame\n",
    "print(\"\\nPreview of the final Spark DataFrame with 'timeseries':\")\n",
    "spark_df_final.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the database and table name in this case I'm using Databricks to create the table\n",
    "database_name = \"default.andrey\"  # Replace with your database name if different\n",
    "table_name = \"volumenes\"\n",
    "\n",
    "# Ensure the database exists\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "# Append the new data to the existing Delta table and enable schema evolution\n",
    "spark_df_final.write.mode(\"append\").format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{database_name}.{table_name}\")\n",
    "\n",
    "#If you are running this for the first time or want to overwrite the table use this code section\n",
    "#spark_df_final.write.mode(\"append\").format(\"delta\") \\\n",
    "#    .option(\"mergeSchema\", \"true\") \\\n",
    "#    .saveAsTable(f\"{database_name}.{table_name}\")\n",
    "\n",
    "print(f\"Data successfully appended to Delta table: {database_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM default.andrey.volumenes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
